{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f03cff31-83e1-4954-9b8e-7b22ffe5e08a"
    }
   },
   "source": [
    "# Sentiment Analysis P1\n",
    "\n",
    "In this notebook, you will learn to \n",
    "- use different packages in Python to build a complete pipeline for solving sentiment analysis problem. \n",
    "\n",
    "- use the simplest model, Mutinomial NB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dadf36f5-94ca-4ad6-82e5-eee766bbe079"
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e28f0511-3ad7-4f7b-a9c2-7667d0f738d3"
    }
   },
   "source": [
    "<img src=\"resources/pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4993d77d-4107-4ee2-b456-537ad84b6927"
    }
   },
   "source": [
    "## Get familiar with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bafb6e90-d675-4161-ab5d-23c386f8762d"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nlp_proj_utils import get_imdb_dataset\n",
    "\n",
    "pd.set_option('max_colwidth', 500)  # Set display column width to show more content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, download if necessary\n",
    "train, test = get_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample (head) of the data frame\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "490495cc-6ecb-4415-b13c-4cd6ea380b6e"
    }
   },
   "outputs": [],
   "source": [
    "print('train shape:', train.shape)\n",
    "print('test  shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8bda0af1-6a24-4cd9-97ec-d4bff3746662"
    }
   },
   "outputs": [],
   "source": [
    "# Statics on tags\n",
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "21638fdf-767a-4ccf-9328-fc01542c924e"
    }
   },
   "source": [
    "See [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/10min.html?highlight=data%20frame) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e2e6ba14-7fea-4c02-88e9-f87688e70238"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Normalization\n",
    "\n",
    "For preprocessing, we will apply the following steps:\n",
    "\n",
    "1. Remove HTML tag (`<br />` in this case) from the review text\n",
    "2. Remove punctuations (replace with whitespace)\n",
    "3. Split review text into tokens\n",
    "4. Remove tokens that are considered as \"**stopwords**\"\n",
    "5. For the rest, do lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# If this is the first time you use nltk, \n",
    "# make sure to download necessary resources and pre-trained models\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0300719b-d5df-4b2a-8b30-97eb486dfd08"
    }
   },
   "outputs": [],
   "source": [
    "transtbl = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dd71b757-9090-4aec-874f-3a972cef6b7c"
    }
   },
   "outputs": [],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8f60988b-d40b-421a-9e35-16471359736e"
    }
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c16c7bfa-f53a-4d52-bfab-cc1d4edad5ea"
    }
   },
   "outputs": [],
   "source": [
    "'ababc'.translate(str.maketrans('abc','def'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6cf378f3-f9a2-466c-8993-e8b08ae1959a"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Take a text input and return the preprocessed string.\n",
    "    i.e.: preprocessed tokens concatenated by whitespace\n",
    "    \"\"\"\n",
    "    line = line.replace('<br />', '').translate(transtbl)\n",
    "    \n",
    "    # list\n",
    "    tokens = [lemmatizer.lemmatize(t.lower(),'v')\n",
    "              for t in nltk.word_tokenize(line)\n",
    "              if t.lower() not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a7867dff-ea5b-41c8-94fb-702b34ebdce1"
    }
   },
   "outputs": [],
   "source": [
    "test_str = \"I bought several books yesterday<br /> and I really love them!\"\n",
    "preprocessing(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using macOS and Linux, you may run un-comment the following code to speed up the preprocessing\n",
    "\n",
    "# !pip install pandarallel\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using Windows, run the following, otherwise, comment this out, and run the second statement instead\n",
    "for df in train, test:\n",
    "    df['text_prep'] = df['text'].progress_apply(preprocessing)\n",
    "    \n",
    "# If you're using macOS or Linux, un-comment and run the following code\n",
    "# for df in train, test:\n",
    "#     df['text_prep'] = df['text'].parallel_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train.shape == (25000, 3)\n",
    "assert test.shape == (25000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4eeefbfd-9a30-4e11-9647-daa037e26042"
    }
   },
   "source": [
    "### Build Vocabulary\n",
    "\n",
    "Instead of using `CountVectorizer` (N-gram) provided by sklearn directly, we will build the vocabulary on our own, so that we have more control over it.\n",
    "\n",
    "<span style=\"color:red\">**Tips:**</span>\n",
    "\n",
    "We can only use words in training data for building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f3befe75-935e-479c-b78e-a739199a7a95"
    }
   },
   "outputs": [],
   "source": [
    "all_words = [w for text in tqdm(train['text_prep']) \n",
    "             for w in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e9e13deb-7fad-47d9-a5d3-9203e14e721c"
    }
   },
   "outputs": [],
   "source": [
    "# Use FreqDist to get count for each word\n",
    "voca = nltk.FreqDist(all_words)\n",
    "print(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e87733fb-34db-482b-a21a-acb88c5b3e93"
    }
   },
   "outputs": [],
   "source": [
    "voca.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9fb9823c-b17b-4a6b-a478-ee9a923148e8"
    }
   },
   "outputs": [],
   "source": [
    "topwords = [word for word, _ in voca.most_common(10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9d86b026-19b4-47e1-a693-88f434dfb59e"
    }
   },
   "source": [
    "### Vectorizer\n",
    "\n",
    "For this section, we will try two ways to do vectorization: **BoW** (1-gram) and **BoW with Tfidf Transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "02fe7ca4-bcdf-4dc3-a066-4def28483597"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, \n",
    "    TfidfTransformer, \n",
    "    TfidfVectorizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eb6794ac-150b-41a7-8b7d-2e9934db9d74"
    }
   },
   "outputs": [],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6c003f7f-2a78-47a2-ada2-a212f75b6517"
    }
   },
   "source": [
    "### Tfâ€“idf Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6e37f508-9785-4250-a1e1-1354dfcff4c2"
    }
   },
   "source": [
    "- Tf: Term-Frequency\n",
    "- idf: Inverse Document-Frequency\n",
    "- Tf-idf = $tf(t,d) \\times idf(t)$\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{1 + n_d}{1 + df(d, t)}} + 1\n",
    "$$\n",
    "\n",
    "![](http://www.onemathematicalcat.org/Math/Algebra_II_obj/Graphics/log_base_gt1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5f349274-78e7-41be-968b-ca1a4fb80d50"
    }
   },
   "source": [
    "> Sentence 1: The boy **love** the toy <br>\n",
    "> Sentence 2: The boy **hate** the toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "12d18c1c-3899-4ea6-a677-2506a5f1d8a0"
    }
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "21537231-5ff9-4a7c-a54f-8ed911c88329"
    }
   },
   "outputs": [],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "90c1cbe3-c5b2-4e80-8c88-4f0ec45b2e8a"
    }
   },
   "outputs": [],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a8bae77b-721c-4fa6-a10b-051574086736"
    }
   },
   "source": [
    "<span style=\"color:red\">**Tips:**</span>\n",
    "\n",
    "tf-idfs are computed slightly different in sklearn, where:\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{n_d}{1 + df(d, t)}}\n",
    "$$\n",
    "\n",
    "With `smooth_idf=True` set to `True`, the formula is:\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{n_d}{df(d, t)}} + 1\n",
    "$$\n",
    "\n",
    "\n",
    "It's always worth trying tfidf transformer for text classification problem. Since `CountVectorizer` and `TfidTransformer` are often chained together, sklearn also provide a class that combines the two steps together: `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e07ae14e-e1e8-4bcd-8986-da2d14b2d3ba"
    }
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the sentences from the slide as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a396d14b-1f38-44db-b25f-d457fe3998a7"
    }
   },
   "outputs": [],
   "source": [
    "t_corpus = ['the boy love the toy', 'the boy hate the toy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "# Voc = ['boy', 'hate', 'love', 'the', 'toy']\n",
    "\n",
    "t_cnt_vec = CountVectorizer()\n",
    "t_cnt_vec.fit(' '.join(t_corpus).split())\n",
    "t_cnt_vec.transform(t_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a2e931ef-7434-4014-9f09-57a2b684f6d5"
    }
   },
   "outputs": [],
   "source": [
    "# Tfidf\n",
    "t_tfidf_vec = TfidfVectorizer()\n",
    "t_tfidf_vec.fit(' '.join(t_corpus).split())\n",
    "t_tfidf_vec.transform(t_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee88a760-c8c0-4c21-92b0-bc6235086445"
    }
   },
   "source": [
    "### Vectorization / Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train['text_prep'], train['sentiment']\n",
    "test_x, test_y = test['text_prep'], test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7c6f787e-5013-466a-869d-bf4b76f681e9"
    }
   },
   "outputs": [],
   "source": [
    "# Use topwords as vocabulary\n",
    "tf_vec = TfidfVectorizer(vocabulary=topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "64ba5bc9-96ea-4829-8a7e-cf5d01781087"
    }
   },
   "outputs": [],
   "source": [
    "train_features = tf_vec.fit_transform(train_x)\n",
    "test_features = tf_vec.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "71abd6d5-05a7-4075-bd7a-f49a07d2f1d7"
    }
   },
   "outputs": [],
   "source": [
    "assert train_features.shape == (25000, 10000)\n",
    "assert test_features.shape == (25000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[0][:50].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ecfbfb7-9b24-499f-a4b4-087e3183b7ee"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "### [Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for **classification with discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eef54d8f-0863-4285-bf5a-fa301dff8678"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "08172515-7b9a-4a38-967b-fcea2ac03b4b"
    }
   },
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "mnb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2e1f0988-c008-441d-a60a-797620243dce"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train Model\n",
    "mnb_model.fit(train_features, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f1a1e3f4-225f-4a59-ab02-82c874289bb1"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "42e5eacf-8353-47df-93db-72ebe1a713a8"
    }
   },
   "outputs": [],
   "source": [
    "print('Accuracy: %f' % metrics.accuracy_score(pred,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Tips:**</span>\n",
    "\n",
    "It doesn't matter if you change the order of `pred` and `test_y` passed into `accuracy_score` since the metrics is symmetric. **However**, it is extremely important that you pass them in the correct order when you need to calculate per-class metrics like f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1dc8a4ad-96e4-41f3-aeda-446c0799c0da"
    }
   },
   "outputs": [],
   "source": [
    "# Pass in as keyword arguments to make sure the order is correct\n",
    "print(\n",
    "    metrics.classification_report(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fcf0efe6-291b-4d03-a7d1-2ec665279f1e"
    }
   },
   "outputs": [],
   "source": [
    "# Example from sklearn documentation\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1bcd8bdb-bdb9-401a-b9ce-e29ccc320f83"
    }
   },
   "source": [
    "## Predict new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1c0addbf-c65c-4d4a-89aa-fab535a72d35"
    }
   },
   "outputs": [],
   "source": [
    "def predict_new(prep_func,  # func for preprocessing\n",
    "                vec,        # vectorizer\n",
    "                model,      # model\n",
    "                text):      # text\n",
    "    \n",
    "    prep_text = prep_func(text)\n",
    "    features = vec.transform([prep_text])\n",
    "    pred = model.predict(features)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "predict_new_p1 = partial(predict_new, preprocessing, tf_vec, mnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "609df79f-52c5-42af-b849-e745afa397f7"
    }
   },
   "outputs": [],
   "source": [
    "predict_new_p1('It looks nice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5ae16efd-71c1-4465-a247-33af0bb9246a"
    }
   },
   "source": [
    "## Tunning hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2d652ee5-fbc8-489b-8003-a79bd96d23db"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_n_topwords(n: int, tfidf=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Train and get the accuracy with different model settings\n",
    "    Args:\n",
    "        n: number of features (top frequent words in the vocabulary)\n",
    "        tfidf: whether do tf-idf re-weighting or not\n",
    "    Outputs:\n",
    "        tuple: (accuracy score, classifier, vectorizer)\n",
    "    \"\"\"\n",
    "    topwords = [word for word, _ in voca.most_common(n)]\n",
    "    \n",
    "    if tfidf:\n",
    "        vec = TfidfVectorizer(vocabulary=topwords)\n",
    "    else:\n",
    "        vec = CountVectorizer(vocabulary=topwords)\n",
    "    \n",
    "    # Generate feature vectors\n",
    "    train_features = vec.fit_transform(train_x)\n",
    "    test_features  = vec.transform(test_x)\n",
    "    \n",
    "    # NB\n",
    "    mnb_model = MultinomialNB()\n",
    "    mnb_model.fit(train_features, train_y)\n",
    "    \n",
    "    # Test predict\n",
    "    pred = mnb_model.predict(test_features)\n",
    "    \n",
    "    return metrics.accuracy_score(pred, test_y), mnb_model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2505b8b5-0bba-4cca-8ba0-93cac0501e3e"
    }
   },
   "outputs": [],
   "source": [
    "train_with_n_topwords(500, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1790e0a2-cf5d-45e4-a19a-e24005be2db1"
    }
   },
   "outputs": [],
   "source": [
    "possible_n = [500 * i for i in range(1, 20)]\n",
    "\n",
    "cnt_accuracies = []\n",
    "tfidf_accuracies = []\n",
    "\n",
    "for n in tqdm(possible_n):\n",
    "    cnt_accuracies.append(train_with_n_topwords(n)[0])\n",
    "    tfidf_accuracies.append(train_with_n_topwords(n, tfidf=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4cb349db-39ae-4311-8521-1e055907abdd"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "plt.plot(possible_n, cnt_accuracies, label='Word Count')\n",
    "plt.plot(possible_n, tfidf_accuracies, label='Tf-idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fb560459-df66-41b6-8a64-842622e4aa79"
    }
   },
   "source": [
    "**Expected**:\n",
    "\n",
    "<img src=\"resources/plot.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "db997a18-0447-456d-a77f-d0f7a8c0b39a"
    }
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model, vec = train_with_n_topwords(3000, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5558a033-1f08-45b4-8abe-ed974d685831"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tf_vec.pkl', 'wb') as fp:\n",
    "    pickle.dump(vec, fp)\n",
    "    \n",
    "with open('mnb_model.pkl', 'wb') as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
